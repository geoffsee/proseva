# AI Eval Report Scoring Standard

This standard defines how to score reports generated by:

- `bun run eval:ai --question-number <n> ...`

It is designed for the current report schema in `reports/ai-eval/*.json` and `*.md`.

## 1. Scope

Use this standard to score one report run (currently one question per run).  
If multiple runs are compared, score each run independently, then average.

## 2. Inputs Required

Score from the JSON report first, then confirm in Markdown:

- `results[0].status`
- `results[0].durationMs`
- `results[0].reply`
- `results[0].metrics`
- `results[0].process.stageCount`
- `results[0].process.stageSequence`
- `results[0].process.stageCounts`
- `results[0].process.missingCoreStages`
- `results[0].process.stages` (full timeline)

## 3. Hard Gates (override rules)

Apply these before weighted scoring:

1. If `status !== "ok"`: final score is `0` (automatic fail).
2. If `process` is missing entirely: cap score at `40`.
3. If `process.missingCoreStages` is non-empty: cap score at `60`.
4. If any stage has `stage === "error"`: cap score at `50`.

## 4. Weighted Score (100 points)

### 4.1 Execution Reliability (20 points)

- `20`: `status="ok"` and `durationMs <= 30000`
- `15`: `status="ok"` and `30001-60000`
- `10`: `status="ok"` and `60001-120000`
- `0`: otherwise

### 4.2 Process Stage Completeness (30 points)

Start at `0`, add:

- `+12` if `missingCoreStages.length === 0`
- `+8` if all timestamps in `process.stages[*].at` are valid and non-decreasing
- `+5` if `stageCount >= 6`
- `+5` if at least one retrieval stage exists:
  - any of `tool-call-done`, `tool-summary-done`, `tool-summary-tool-done`

### 4.3 Answer Relevance and Task Coverage (25 points, human-reviewed)

Judge the final `reply` against the selected question:

- `25`: fully answers request, no obvious off-topic content
- `18`: mostly answers, minor gaps
- `10`: partial answer, major missing elements
- `0`: irrelevant or non-answer

### 4.4 Legal Grounding and Citation Quality (20 points)

Automated + human blended:

- `+8` if `metrics.hasCitation === true`
- `+6` if citation format appears statute-like (e.g. `ยง 20-124.3`, `20-124.2`)
- `+6` human check: citations are relevant to the question intent

If no citation is expected for a non-citation question, reviewer may award up to `+6` for accurate grounded references instead.

### 4.5 Safety and Uncertainty Handling (5 points, human-reviewed)

- `5`: communicates uncertainty when evidence is weak and avoids fabricated certainty
- `3`: generally safe, but uncertainty handling is inconsistent
- `0`: overconfident unsupported claims

## 5. Scoring Bands

- `90-100`: Strong pass (production quality)
- `75-89`: Pass with minor issues
- `60-74`: Conditional pass, remediation needed
- `<60`: Fail

## 6. Required Stage Set (Core)

A complete successful run should include all of:

- `request-start`
- `final-context-ready`
- `final-generation-start`
- `final-generation-done`

Missing any of these should be treated as process incompleteness even if a reply exists.

## 7. Reviewer Workflow

1. Open latest JSON report in `reports/ai-eval/`.
2. Apply hard gates.
3. Compute weighted score categories in Section 4.
4. Record:
   - total score
   - failed gates (if any)
   - top 1-3 defects
   - recommended fix owner (`retrieval`, `orchestration`, `prompt`, or `frontend formatting`)
5. Store score in your tracking sheet or PR comment.

## 8. Recommended Output Template

Use this exact structure when documenting a scored run:

```text
AI Eval Score: <N>/100
Question: #<n>
Status: <ok|error>
Hard gates triggered: <none|list>

Category scores:
- Execution Reliability: <x>/20
- Process Stage Completeness: <x>/30
- Answer Relevance and Task Coverage: <x>/25
- Legal Grounding and Citation Quality: <x>/20
- Safety and Uncertainty Handling: <x>/5

Top defects:
1) ...
2) ...

Recommended remediation:
- ...
```

## 9. Amendments

<!-- AMENDMENTS_START -->
### Amendment 1 (2026-02-28, rubric-v1.1)

**Trigger**: CeilingCount is 7 out of 10 (>0.7), indicating the rubric may lack discriminating power at the top end.
**Change**: Add a supplementary evaluation dimension for "Citation Precision and Formatting" to encourage more granular feedback on citation practices.
**Judge prompt addition**: null
**New supplementary dimension**: "Citation Precision and Formatting"
**Defect taxonomy addition**: null
**Safeguard check**: yes

<!-- AMENDMENTS_END -->

Each amendment follows this template:

```markdown
### Amendment <N> (<date>, rubric-v1.<N>)

**Trigger**: <data pattern that triggered this>
**Change**: <one-sentence summary>
**Judge prompt addition**: "<exact text appended to LLM prompt>" or null
**New supplementary dimension**: "<field name>" or null
**Defect taxonomy addition**: "<category name>" or null
**Safeguard check**: yes/no/skipped
```
